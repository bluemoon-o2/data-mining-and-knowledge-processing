import os
import json
from db_utils import init_db, save_docs_to_db, get_doc_count, get_all_docs_minimal
from datasets import load_dataset

# è®¾ç½® HuggingFace é•œåƒä»¥åŠ é€Ÿä¸‹è½½
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')
output_json_path = os.path.join(DATA_DIR, 'processed_data.json')
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50


def split_text(text, chunk_size=512, chunk_overlap=50):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆæŒ‡å®šå¤§å°çš„å—ï¼Œå¹¶å¸¦æœ‰é‡å ã€‚
    """
    if not text:
        return []

    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - chunk_overlap
        if start >= len(text):
             break
    
    return [c.strip() for c in chunks if c.strip()]



def run_preprocessing(huatuo_limit=200000, force_refresh=False):
    """
    ç›´æ¥ä» HuggingFace æ•°æ®é›†åŠ è½½å¹¶å¤„ç†ï¼Œå­˜å…¥æ•°æ®åº“ã€‚
    æ”¯æŒæ•°æ®åº“ç¼“å­˜ï¼šå¦‚æœ DB ä¸­å·²æœ‰æ•°æ®ï¼Œåˆ™è·³è¿‡å¤„ç†ã€‚
    """
    import sys
    if "--force_refresh" in sys.argv:
        force_refresh = True
        
    print(f"ğŸš€ å¼€å§‹é¢„å¤„ç†æ£€æŸ¥...")
    os.makedirs(DATA_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰æ•°æ® (å¦‚æœå°‘äº 1000 æ¡ï¼Œå¯èƒ½æ•°æ®ä¸å®Œæ•´ï¼Œå¼ºåˆ¶é‡æ–°å¤„ç†)
    current_count = get_doc_count()
    if current_count > 1000 and not force_refresh:
        print(f"ğŸ“¦ æ•°æ®åº“å·²å­˜åœ¨ {current_count} æ¡æ•°æ®ï¼Œè·³è¿‡æ•°æ®é›†ä¸‹è½½ã€‚")
        
        # æ£€æŸ¥ JSON ç´¢å¼•æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä» DB å¯¼å‡º
        if not os.path.exists(output_json_path):
            print("æ­£åœ¨ä»æ•°æ®åº“å¯¼å‡ºç²¾ç®€ JSON ç´¢å¼•...")
            all_data_minimal = get_all_docs_minimal()
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(all_data_minimal, f, ensure_ascii=False)
            print(f"âœ… JSON ç´¢å¼•å¯¼å‡ºå®Œæˆã€‚")
        return

    all_data = []
    chunk_count = 0

    print(f"æ­£åœ¨ä» HuggingFace åŠ è½½ Huatuo26M-Lite æ•°æ®é›†...")
    try:
        # ä½¿ç”¨é»˜è®¤ç¼“å­˜è·¯å¾„
        ds = load_dataset(
            "FreedomIntelligence/Huatuo26M-Lite", 
            split='train', 
            streaming=False
        )
        
        huatuo_count = 0
        total_to_process = min(len(ds), huatuo_limit)
        print(f"æ‰¾åˆ°æ•°æ®é›†è®°å½• {len(ds)} æ¡ï¼Œå‡†å¤‡å¤„ç†å‰ {total_to_process} æ¡...")

        for i in range(total_to_process):
            entry = ds[i]
            question = entry.get('question', '')
            answer = entry.get('answer', '')
            
            if not question or not answer:
                continue
            
            title = f"åŒ»ç–—é—®ç­”ï¼š{question}" # ç§»é™¤ 20 å­—ç¬¦é™åˆ¶ï¼Œå­˜å‚¨å…¨é•¿é—®é¢˜
            content = f"é—®é¢˜ï¼š{question}\nå›ç­”ï¼š{answer}"
            
            # æ–‡æœ¬åˆ†å‰²
            chunks = split_text(content, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
            
            for idx, chunk in enumerate(chunks):
                chunk_count += 1
                data_entry = {
                    "id": chunk_count - 1,
                    "title": title,
                    "content": chunk,
                    "source_file": "Huatuo26M-Lite (Cached)",
                    "chunk_index": idx
                }
                all_data.append(data_entry)
            
            huatuo_count += 1
            if huatuo_count % 10000 == 0:
                print(f"  å·²å¤„ç† {huatuo_count} / {total_to_process} æ¡è®°å½•...")
        
        print(f"âœ… æ•°æ®é›†è®°å½•å¤„ç†å®Œæˆï¼Œå…±è®¡ {huatuo_count} æ¡ï¼Œç”Ÿæˆ {chunk_count} ä¸ªæ–‡æœ¬å—ã€‚")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æˆ–å¤„ç†æ•°æ®é›†æ—¶å‡ºé”™: {e}")
        return

    # 2. ä¿å­˜åˆ°æ•°æ®åº“ (SQLite)
    print("æ­£åœ¨ä¿å­˜æ–‡æœ¬å†…å®¹åˆ°æ•°æ®åº“...")
    save_docs_to_db(all_data)
    
    # 3. ä¿å­˜ç²¾ç®€ç‰ˆç´¢å¼• (JSON) ä¾› FAISS å¯åŠ¨æ—¶å¿«é€Ÿæ„å»º ID æ˜ å°„
    print(f"æ­£åœ¨ä¿å­˜ç²¾ç®€ç´¢å¼•åˆ°: {output_json_path}")
    try:
        with open(output_json_path, 'w', encoding='utf-8') as f:
            # åªä¿ç•™ id å’Œç”¨äºæ£€ç´¢çš„ content å­—æ®µï¼Œæå¤§å‡å° JSON ä½“ç§¯
            minimal_data = [{"id": d["id"], "content": d["content"]} for d in all_data]
            json.dump(minimal_data, f, ensure_ascii=False)
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼æ€»å—æ•°: {chunk_count}")
    except Exception as e:
        print(f"é”™è¯¯ï¼šæ— æ³•å†™å…¥ JSON æ–‡ä»¶: {e}")

if __name__ == "__main__":
    run_preprocessing(huatuo_limit=200000)
