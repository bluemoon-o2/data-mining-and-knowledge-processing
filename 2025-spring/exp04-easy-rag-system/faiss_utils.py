import faiss
import numpy as np
import os
import json
from typing import Dict, List, Tuple

from config import (
    ADAPTIVE_DISTANCE_GAP_THRESHOLD,
    ADAPTIVE_TOPK_MAX,
    ADAPTIVE_TOPK_MIN,
    EMBEDDING_DIM,
    HYBRID_DENSE_K,
    HYBRID_RRF_K0,
    HYBRID_SPARSE_K,
    INDEX_PARAMS,
    INDEX_TYPE,
    SEARCH_PARAMS,
    TOP_K,
    USE_ADAPTIVE_TOPK,
    USE_HYBRID_RETRIEVAL,
)
from db_utils import ensure_fts_index, fts_search, get_docs_by_ids

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(CURRENT_DIR, "data")
FAISS_INDEX_PATH = os.path.join(DATA_DIR, "faiss_index.bin")
DOC_MAP_PATH = os.path.join(DATA_DIR, "id_to_doc_map.json")
_FTS_READY: bool | None = None

if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

class FAISSClient:
    def __init__(self, dimension):
        self.dimension = dimension
        self.index_type = INDEX_TYPE
        self.index = None
        self.doc_count = 0

    def _init_ivf_index(self, num_samples):
        # åŠ¨æ€è°ƒæ•´ nlistï¼Œç¡®ä¿æ»¡è¶³ FAISS çš„è®­ç»ƒè¦æ±‚ (nx >= nlist)
        # å»ºè®®æ¯ä¸ªèšç±»ä¸­å¿ƒè‡³å°‘æœ‰ 30-100 ä¸ªç‚¹ï¼Œä½†æœ€å°å¿…é¡»æ»¡è¶³ 1:1
        target_nlist = INDEX_PARAMS.get("nlist", 1024)
        if num_samples < target_nlist:
            print(f"âš ï¸ æ•°æ®é‡ ({num_samples}) å°äºé¢„è®¾èšç±»æ•° ({target_nlist})ï¼Œè‡ªåŠ¨è°ƒæ•´ nlist ä¸º {max(1, num_samples // 10)}")
            nlist = max(1, num_samples // 10)
        else:
            nlist = target_nlist
            
        quantizer = faiss.IndexFlatL2(self.dimension)
        self.index = faiss.IndexIVFFlat(quantizer, self.dimension, nlist, faiss.METRIC_L2)
        print(f"âœ… IVF ç´¢å¼•åˆå§‹åŒ–å®Œæˆ (nlist={nlist})")

    def add_documents(self, embeddings):
        if len(embeddings) == 0:
            return
        embeddings = np.array(embeddings).astype('float32')
        
        # å¦‚æœç´¢å¼•å°šæœªåˆå§‹åŒ– (é’ˆå¯¹ IVF)
        if self.index is None:
            if self.index_type == "IVF_FLAT":
                self._init_ivf_index(len(embeddings))
            else:
                self.index = faiss.IndexFlatL2(self.dimension)
        
        if isinstance(self.index, faiss.IndexIVFFlat) and not self.index.is_trained:
            print(f"æ­£åœ¨è®­ç»ƒ IVF ç´¢å¼•ï¼ˆæ ·æœ¬æ•°: {len(embeddings)}ï¼‰...")
            self.index.train(embeddings)
            
        self.index.add(embeddings)
        self.doc_count += len(embeddings)

    def search(self, query_emb, k=TOP_K):
        query_emb = np.array([query_emb]).astype('float32')
        # è®¾ç½®æœç´¢å‚æ•° (nprobe)
        if isinstance(self.index, faiss.IndexIVFFlat):
            self.index.nprobe = SEARCH_PARAMS.get("nprobe", 10)
            
        distances, indices = self.index.search(query_emb, k)
        return indices[0], distances[0]

    def save(self, index_path, map_path):
        faiss.write_index(self.index, index_path)
        # ä¸å†ä¿å­˜å®Œæ•´çš„ doc map åˆ° JSONï¼Œåªä¿å­˜æ•°é‡æˆ–è½»é‡å…ƒæ•°æ®
        with open(map_path, 'w', encoding='utf-8') as f:
            json.dump({"doc_count": self.doc_count}, f)

    def load(self, index_path, map_path):
        if os.path.exists(index_path):
            try:
                self.index = faiss.read_index(index_path)
                self.doc_count = self.index.ntotal
                print(f"âœ… ä» {index_path} åŠ è½½äº†åŒ…å« {self.doc_count} æ¡å‘é‡çš„ç´¢å¼•")
                return True
            except Exception as e:
                print(f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
                return False
        return False

def get_faiss_client():
    """åˆå§‹åŒ–å¹¶è¿”å› FAISS å®¢æˆ·ç«¯"""
    return FAISSClient(EMBEDDING_DIM)

def index_data_if_needed(client, embedding_model):
    """å¦‚æœéœ€è¦ï¼Œä»æ•°æ®åº“è¯»å–æ•°æ®å¹¶è¿›è¡Œç´¢å¼•"""
    # 1. å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
    if client.load(FAISS_INDEX_PATH, DOC_MAP_PATH) and client.doc_count > 0:
        return True

    print("ğŸ” æœªæ‰¾åˆ°æœ‰æ•ˆç´¢å¼•ï¼Œæ­£åœ¨ä»æ•°æ®åº“è¯»å–æ•°æ®å¹¶æ„å»ºç´¢å¼•...")
    from db_utils import get_doc_count, get_all_docs_minimal
    
    total_docs = get_doc_count()
    if total_docs == 0:
        print("âŒ æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œ preprocess.py")
        return False

    print(f"æ­£åœ¨ç´¢å¼• {total_docs} æ¡æ–‡æ¡£ï¼ˆæ‰¹å¤„ç†æ¨¡å¼ï¼‰...")
    
    # åˆ†æ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜
    BATCH_SIZE = 10000
    all_docs = get_all_docs_minimal()
    
    for i in range(0, len(all_docs), BATCH_SIZE):
        batch = all_docs[i:i+BATCH_SIZE]
        texts = [doc['content'] for doc in batch]
        embeddings = embedding_model.encode(texts, show_progress_bar=False)
        client.add_documents(embeddings)
        print(f"  å·²ç´¢å¼• {min(i + BATCH_SIZE, total_docs)} / {total_docs}...")

    client.save(FAISS_INDEX_PATH, DOC_MAP_PATH)
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œå…± {client.doc_count} æ¡ã€‚")
    return True

def search_similar_documents(client, query, embedding_model, k=TOP_K):
    """åœ¨ FAISS ä¸­æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼Œå¹¶ä»æ•°æ®åº“è·å–è¯¦ç»†å†…å®¹"""
    if not client or client.doc_count == 0:
        return [], []
    
    # BGE æ¨¡å‹å»ºè®®æ·»åŠ æŸ¥è¯¢æŒ‡ä»¤ä»¥æå‡æ£€ç´¢æ•ˆæœ
    query = f"ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š{query}"

    query_emb = embedding_model.encode([query])[0]
    indices, distances = client.search(query_emb, k=k)
    
    # è¿‡æ»¤æ‰æ— æ•ˆç´¢å¼•
    valid_results = [(idx, dist) for idx, dist in zip(indices, distances) if idx != -1]
    if not valid_results:
        print("âš ï¸ FAISS æœªè¿”å›ä»»ä½•æœ‰æ•ˆç´¢å¼• (æ‰€æœ‰ç»“æœå‡ä¸º -1)")
        return [], []
        
    ids = [int(idx) for idx, dist in valid_results]
    distances = [float(dist) for idx, dist in valid_results]
    
    # ä»æ•°æ®åº“ä¸­æŒ‰éœ€æ‹‰å–æ–‡æ¡£å†…å®¹
    docs = get_docs_by_ids(ids)
    if not docs:
        print(f"âš ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ° ID åˆ—è¡¨å¯¹åº”çš„æ–‡æ¡£: {ids}")
    else:
        print(f"âœ… æˆåŠŸä»æ•°æ®åº“è·å– {len(docs)} æ¡æ–‡æ¡£å†…å®¹")
    
    return docs, distances


def _rrf_fuse(rank_lists: List[List[int]], *, k0: int, topk: int) -> List[int]:
    scores: Dict[int, float] = {}
    for ranked in rank_lists:
        for r, doc_id in enumerate(ranked, start=1):
            scores[doc_id] = scores.get(doc_id, 0.0) + 1.0 / float(k0 + r)
    fused = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return [int(doc_id) for doc_id, _ in fused[:topk]]


def _choose_adaptive_topk(*, query: str, distances: List[float]) -> int:
    if not USE_ADAPTIVE_TOPK:
        return TOP_K
    if not distances:
        return TOP_K
    qlen = len("".join((query or "").split()))
    if qlen <= 12:
        return int(ADAPTIVE_TOPK_MIN)
    if len(distances) >= 2:
        gap = float(distances[1]) - float(distances[0])
        if gap >= float(ADAPTIVE_DISTANCE_GAP_THRESHOLD):
            return int(ADAPTIVE_TOPK_MIN)
    return int(ADAPTIVE_TOPK_MAX)


def search_documents(
    client,
    query: str,
    embedding_model,
    *,
    topk: int = TOP_K,
    enable_hybrid: bool = USE_HYBRID_RETRIEVAL,
) -> Tuple[List[dict], List[float]]:
    if not enable_hybrid:
        return search_similar_documents(client, query, embedding_model, k=topk)

    global _FTS_READY
    if _FTS_READY is None:
        _FTS_READY = bool(ensure_fts_index())
    ok = bool(_FTS_READY)
    dense_k = max(int(topk), int(HYBRID_DENSE_K))
    sparse_k = max(int(topk), int(HYBRID_SPARSE_K))

    dense_docs, dense_dist = search_similar_documents(client, query, embedding_model, k=dense_k)
    dense_ids = []
    dense_id_to_dist: Dict[int, float] = {}
    for d, dist in zip(dense_docs, dense_dist):
        doc_id = int(d.get("id", -1)) if isinstance(d, dict) and "id" in d else None
        if doc_id is None:
            continue
        dense_ids.append(doc_id)
        dense_id_to_dist[doc_id] = float(dist)

    sparse_ids: List[int] = []
    if ok:
        sparse_rows = fts_search(query, sparse_k)
        sparse_ids = [int(doc_id) for doc_id, _ in sparse_rows]

    fused_ids = _rrf_fuse([dense_ids, sparse_ids], k0=int(HYBRID_RRF_K0), topk=int(topk))
    docs = get_docs_by_ids(fused_ids)
    distances = [dense_id_to_dist.get(int(doc_id), float("nan")) for doc_id in fused_ids]
    return docs, distances


def retrieve_with_adaptive_topk(client, query: str, embedding_model) -> Tuple[List[dict], List[float], int]:
    docs, dist = search_documents(client, query, embedding_model, topk=max(int(ADAPTIVE_TOPK_MAX), int(TOP_K)))
    chosen = _choose_adaptive_topk(query=query, distances=[float(x) for x in dist if x == x])
    return docs[:chosen], dist[:chosen], int(chosen)
