import os
import time
import json
import uvicorn
from fastapi import FastAPI
from fastapi.responses import HTMLResponse, StreamingResponse
from pydantic import BaseModel
from typing import List
from dotenv import load_dotenv

load_dotenv()

# é…ç½® HuggingFace é•œåƒ (åŠ é€Ÿæœ¬åœ°åµŒå…¥æ¨¡å‹åŠ è½½)
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from config import (
    ABSTAIN_MAX_DISTANCE,
    ABSTAIN_MIN_DOCS,
    EMBEDDING_MODEL_NAME,
    RERANK_MODEL_NAME,
    TOP_K,
    USE_ADAPTIVE_TOPK,
    USE_EVIDENCE_ABSTAIN,
    USE_HYBRID_RETRIEVAL,
    USE_RERANKER,
)
from models import load_embedding_model, load_rerank_model
from faiss_utils import get_faiss_client, index_data_if_needed, retrieve_with_adaptive_topk, search_documents
from rag_core import generate_answer, generate_answer_stream

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

# 1. åˆå§‹åŒ– RAG ç´¢å¼• (æœ¬åœ°åµŒå…¥æ¨¡å‹ + å‘é‡æ•°æ®åº“)
print("\n" + "="*50)
print("ğŸ” æ­£åœ¨åˆå§‹åŒ– RAG æ£€ç´¢ç¯å¢ƒ...")
print("="*50)

faiss_client = get_faiss_client()
embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
rerank_model = None
if USE_RERANKER:
    rerank_model = load_rerank_model(RERANK_MODEL_NAME)

# ç›´æ¥ä»æ•°æ®åº“å’Œç´¢å¼•åˆå§‹åŒ–
index_data_if_needed(faiss_client, embedding_model)

# 2. å¯åŠ¨ FastAPI
app = FastAPI(title="Medical RAG API (Cloud Version)")

class QueryRequest(BaseModel):
    query: str

class QueryResponse(BaseModel):
    answer: str
    context: List[dict]
    time_taken: float
    stats: dict = None


async def retrieve_docs(query: str, is_rerank: bool = False) -> List[dict]:
    initial_k = TOP_K * 5 if is_rerank else TOP_K
    try:
        if USE_ADAPTIVE_TOPK and not is_rerank:
            docs, _, _ = retrieve_with_adaptive_topk(faiss_client, query, embedding_model)
            retrieved_docs = docs
        else:
            retrieved_docs, _ = search_documents(
                faiss_client,
                query,
                embedding_model,
                topk=int(initial_k),
                enable_hybrid=bool(USE_HYBRID_RETRIEVAL),
            )
        # è¿‡æ»¤ç©ºå†…å®¹æ–‡æ¡£ï¼Œæå‡åç»­å›ç­”è´¨é‡
        retrieved_docs = [doc for doc in retrieved_docs if doc.get("content", "").strip()]
        return retrieved_docs
    except Exception as e:
        print(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
        return []

@app.post("/query/stream")
async def query_rag_stream(request: QueryRequest):
    print(f"\n[Stream] æ”¶åˆ°é—®é¢˜: {request.query}")
    
    # 1. æœç´¢ FAISS å¹¶è·å–æ–‡æ¡£
    docs = await retrieve_docs(request.query)
    
    if not docs:
        async def empty_gen():
            yield json.dumps({"error": "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"}) + "\n"
        return StreamingResponse(empty_gen(), media_type="text/event-stream")
    
    # 2. å®šä¹‰ç”Ÿæˆå™¨
    async def stream_generator():
        try:
            # é¦–å…ˆå‘é€ä¸Šä¸‹æ–‡ä¿¡æ¯
            yield json.dumps({"context": docs}, ensure_ascii=False) + "\n"
            
            # ä½¿ç”¨ run_in_threadpool é˜²æ­¢é˜»å¡äº‹ä»¶å¾ªç¯
            from starlette.concurrency import iterate_in_threadpool
            async for chunk_data in iterate_in_threadpool(generate_answer_stream(request.query, docs)):
                if chunk_data:
                    yield json.dumps({
                        "answer_chunk": chunk_data["text"],
                        "token_count": chunk_data["token_count"],
                        "speed": chunk_data["speed"],
                        "elapsed": chunk_data["elapsed"]
                    }, ensure_ascii=False) + "\n"
        except Exception as e:
            print(f"Streaming Error: {e}")
            yield json.dumps({"error": str(e)}, ensure_ascii=False) + "\n"
                
    return StreamingResponse(stream_generator(), media_type="text/event-stream")

@app.post("/query", response_model=QueryResponse)
async def query_rag(request: QueryRequest):
    # è¿‡æ»¤å‰ç«¯å¿ƒè·³æ£€æµ‹è¯·æ±‚
    if request.query.lower() == "ping":
        return {
            "answer": "pong",
            "context": [],
            "time_taken": 0.0,
            "stats": {"doc_count": faiss_client.doc_count}
        }
    
    print(f"\n[Query] æ”¶åˆ°é—®é¢˜: {request.query}")
    start_time = time.time()
    
    # 1. æœç´¢ FAISS å¹¶è·å–æ–‡æ¡£
    docs = await retrieve_docs(request.query, is_rerank=USE_RERANKER)
    
    # 2. é‡æ’åº
    if rerank_model and docs:
        print(f"æ­£åœ¨é‡æ’åº {len(docs)} æ¡æ–‡æ¡£...")
        sentence_pairs = [[request.query, doc['content']] for doc in docs]
        scores = rerank_model.predict(sentence_pairs)
        for i, doc in enumerate(docs):
            doc['score'] = float(scores[i])
        docs.sort(key=lambda x: x['score'], reverse=True)
        docs = docs[:TOP_K]
    
    if not docs:
        print("[Query] æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
        return QueryResponse(answer="æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚", context=[], time_taken=time.time() - start_time)

    if USE_EVIDENCE_ABSTAIN:
        top_dist = None
        try:
            _, dists = search_documents(
                faiss_client,
                request.query,
                embedding_model,
                topk=1,
                enable_hybrid=False,
            )
            if dists:
                top_dist = float(dists[0])
        except Exception:
            top_dist = None
        if len(docs) < int(ABSTAIN_MIN_DOCS) or (top_dist is not None and top_dist > float(ABSTAIN_MAX_DISTANCE)):
            return QueryResponse(
                answer="å½“å‰çŸ¥è¯†åº“æœªæ£€ç´¢åˆ°è¶³å¤Ÿå¯é çš„è¯æ®æ¥æ”¯æŒè¯¥é—®é¢˜çš„å›ç­”ã€‚å»ºè®®è¡¥å……å…³é”®ä¿¡æ¯æˆ–å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚",
                context=docs,
                time_taken=time.time() - start_time,
                stats={"abstained": True, "top_distance": top_dist, "hybrid": bool(USE_HYBRID_RETRIEVAL)},
            )
    
    print(f"[Query] æ£€ç´¢åˆ° {len(docs)} æ¡ç›¸å…³ä¸Šä¸‹æ–‡ã€‚")
    
    # 2. ç”Ÿæˆç­”æ¡ˆ
    print("[Query] æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
    result = generate_answer(request.query, docs)
    print("[Query] ç­”æ¡ˆç”Ÿæˆå®Œæˆã€‚")

    merged_stats = {"hybrid": bool(USE_HYBRID_RETRIEVAL), "adaptive_topk": bool(USE_ADAPTIVE_TOPK), "abstained": False}
    if isinstance(result, dict) and isinstance(result.get("stats"), dict):
        merged_stats.update(result["stats"])
    
    return QueryResponse(
        answer=result["answer"], 
        context=docs, 
        time_taken=time.time() - start_time,
        stats=merged_stats
    )

@app.get("/", response_class=HTMLResponse)
async def get_index():
    print("[Web] è®¿é—®é¦–é¡µ")
    index_path = os.path.join(CURRENT_DIR, "index.html")
    with open(index_path, "r", encoding="utf-8") as f:
        return f.read()

if __name__ == "__main__":
    print("\n" + "="*50)
    print("ğŸš€ æ‰€æœ‰å‡†å¤‡å°±ç»ªï¼")
    print("ğŸ‘‰ è¯·åœ¨æµè§ˆå™¨è®¿é—®: http://127.0.0.1:8001")
    print("="*50)
    uvicorn.run(app, host="127.0.0.1", port=8001, workers=1)
